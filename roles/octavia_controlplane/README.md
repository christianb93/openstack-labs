octavia_controlplane
=========

This role install the Octavia control plane (controller worker, health manager, housekeeping) on a node. This needs to be a node where an integration bridge managed by Neutron is deployed, as we will attach to the integration bridge to connect to the load balancer network

## Certificate setup

Certificates for Octavia are a bit tricky. The controller and the agent on each amphora communicate with each other via a TSL secured REST API. The client certificate and client key used for this communication is static and needs to be created by the administrator during the installation. The server certificates, however, are created automatically by Octavia when an amphora is brought up and mapped into the amphora via a config drive. To be able to do this, Octavia has a built-in certificate generator that acts as a cA. 

* a first CA certificate (octavia_ca). This certificate will be used by the certificate generator and will therefore be the CA certificate with which all server certificates are signed and will be used as a client CA certificate by the Octavia control plane when building a connection to the amphora agent
* the private key for this certificate
* a second CA certificate (install_ca). This certificate will be used during installation to create the client certificates, and consequently needs to be deployed to the amphorae to be able to verify connections. Thus this certificate will be used as client CA inside the amphora.
* a private key for this certificate
* a client certificate (client_cert), signed by the install_ca, which is then used by the control plane, combined with the corresponding key in one PEM file


Thus we have the following table that summarizes the various certificates and keys and their role on the respective nodes.

| Node | Role | Certificate |
| --- | --- | --- |
| Amphora | client CA | install_ca |
| Amphora | server certificate | generated by Octavia and signed with octavia_ca |
| Amphora | server private key | generated by Octavia |
| Control plane | client certificate | client_cert |
| Control plane | client private key | client_key |
| Control plane | server CA | octavia_ca |
| Certificate generator | root CA | octavia_ca |
| Certificate generator | private key | octavia_key |

We will create these certificates on the local host and distribute them, on the Octavia nodes, they will be kept in */etc/octavia/certs/*. This is done by the role prepareCredentials which should be run BEFORE this role executes. 

## Network setup

The Octavia Amphorae communicate with the control plane via a dedicated virtual (!) network called the **load balancer network**. This can be any type of virtual network that Neutron offers, but we will use a VXLAN network. As this network needs to be reachable from the control plane, we run the controller on the network node and attach an OVS port to the integration bridge. 

As the traffic on the integration bridge is tagged with the local VLAN ID of the virtual network, this needs to be an access port, and we need to figure out the local VLAN ID corresponding to the load balancer network. This can be done by locating the port of the DHCP agent that Neutron creates for our network.

Note that this only works because this agent is running on the same node. Thus in a setup with more than one network node, this is not the best possible solution, and we would probably realize the load balancer network as a Neutron flat network, supported by OVS bridges managed outside of OpenStack which are connected via GRE or VXLAN, as we do it for the br-ext bridges.



## Configuration file changes

The following changes are made to the default configuration:

Changes done for the API server as well (though they are not all needed, we do this to allow for a configuration were control plane and API server are installed on the same node)

* set the URL for the RabbitMQ interface
* change the *bind_host* variable to the management IP of the node on which we are running
* change the auth strategy to keystone
* only expose the v2 API
* change the values in the auth_token section 
* set a topic name in oslo_messaging
* in the service_auth section, configure the authentication data that Octavia will use to communicate with other services (though this is apparantly not really needed by the API service itself)
* modify *compute_driver* so that the Nova driver is used. 
* modify *network_driver* to use the allowed address pairs driver


Additional changes:

* set the *cert_manager* so that the local cert manager is used (and we do not need Barbican)
* set all certificates according to the table above
* set *server_certs_key_passphrase*, this really needs to be 32 bytes long otherwise we get an error when trying to create an amphora
* set the *amp_ssh_key_name* to amphora-key, this is the name of the key Nova will place on each newly created amphora
* set the *amp_flavor_id* to the ID of the flavor that we have created for the amphora image
* set the *amp_image_tag* to the tag that we use to identify an image ("amphora"), and set the *amp_image_owner_id* to the id of the service project (not the octavia user!)
* set the *amp_boot_network_list* to the UUID of the load balancer network that we have created. When an amphora instance is [created](https://github.com/openstack/octavia/blob/9904b26a9c40f29f554c56e9e65f6396caa8fea9/octavia/compute/drivers/nova_driver.py#L147), this list is [retrieved](https://github.com/openstack/octavia/blob/9904b26a9c40f29f554c56e9e65f6396caa8fea9/octavia/controller/worker/tasks/compute_tasks.py#L60) from the configuration and the instance is attached to each network contained in it, and when I interpret [this piece of code](https://github.com/openstack/octavia/blob/9904b26a9c40f29f554c56e9e65f6396caa8fea9/octavia/compute/drivers/nova_driver.py#L231) correctly, then the first network in this list is considered to be "the" load balancer network from Octavias point of view. 
* populate *amp_secgroup_list* to apply the load balancer network security group to the amphorae
* set the *amphora_driver* to the amphora_haproxy_rest_driver (the standard REST driver coming with Octavia)
* remark: we do not change the default (noop driver) for the volume driver, so that Octavia will boot the instances from the disk image
* in the glance section, we point Octavia to the Glance service in our only region (RegionOne)
* in the haproxy_amphora section, we set the bind host to 0.0.0.0, this is the IP address to which the agent will bind on the amphora
* in the health_manager section, we need to change the item *bind_ip* which determines on which IP address the health manager will listen on our control plane node to pick up heartbeats from the amphorae, this needs to be an IP address within the load balancer network. We also change *controller_ip_port_list* to match this IP address, as this is the configuration item that the agent on the amphora will look up to determine to which addresses it should send heartbeats (this could be more than one health manager in a HA setup)
* we set the item *heartbeat_key* which is a key used to verify heart beats received from the amphorae
* in the neutron and nova sections, we make the same changes as in the glance section

Requirements
------------

None

Role Variables
--------------

The following variables need to be set when calling this role.

* db_node - name of the node on which the database is running
* api_node - the node on which Keystone is running
* memcached_node - node on which the memcached server is running
* mariadb_root_password - root password for the database
* octavia_db_user_password - a password that we will use for the octavia DB user
* keystone_admin_password - the Keystone admin password
* octavia_keystone_user_password - a password for the new octavia user in Keystone
* mq_node - the node on which RabbitMQ is running
* rabbitmq_password - password of the RabbitMQ user
* amphora_image_url - URL of the Amphora image to use
* lb_port_mtu - the MTU to use for the port that we create to access the load balancer network
* lb_network_cidr - the CIDR of the load balancer network
* lb_network_allocation_start, lb_network_allocation_end - start and end IP address of load balancer network
*  lb_network_gateway_ip - IP address reserved for gateway of load balancer network


Dependencies
------------

We use some of the files prepared by the role prepareCredentials


License
-------

MIT

Author Information
------------------

Visit me at https://www.github.com/christianb93
