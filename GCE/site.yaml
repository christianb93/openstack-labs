# This play will run Terraform via the Terraform module
# and build a dynamic inventory from its output
- name: Run Terraform and build inventory dynamically
  hosts: localhost
  roles:
  - terraform

# Next, we build a local SSH configuration. This is not necessarily needed, but
# makes it easy to SSH into the machines using their logical name
- name: Build SSH configuration
  hosts: localhost
  vars:
    ansible_ssh_private_key_file: "~/.ssh/gcp-default-key"
  roles:
  - sshConfig

 
# This play is only there to make sure that all machines are available before
# we proceed
- name: Wait for all machines to become ready
  hosts: all
  gather_facts: no
  tasks:
  - name: Wait for machine to become reachable
    wait_for_connection:
      delay: 10
      sleep: 10


# Set up APT cache on controller node
- name: Install APT cache on controller
  hosts: controller_nodes
  become: yes
  tasks: 
  - name: Install apt-cacher-ng 
    apt:
      name: apt-cacher-ng
      force_apt_get: yes 
      update_cache: yes
      state: latest

###########################################################################################
# Now we simply include the actual Lab. Note that we really have to 
# put the contents of the playbook here, as using import_playbook or
# include will make Ansible use the group_vars in the lab folder, not the
# group_vars in this folder
###########################################################################################

#
# Create credentials
#
- name: Create credentials
  hosts: localhost
  vars_files:
    - global_vars.yaml
  roles:
    - prepareCredentials

# Make sure that all hosts are up before proceeding
- name: Check connectivity
  hosts: all
  tasks:
    - name: Check connectivity
      ping:



#
# Now import the credentials just created. We cannot
# include this in vars_files as the file is not yet
# present when we run the playbook.
#
- name: Import credentials
  hosts: all
  gather_facts: no
  vars_files:
    global_vars.yaml
  tasks:
    - name: Read credentials
      include_vars:
        file: "{{credentials_dir}}/credentials.yaml"

#
# Perform basic setup steps on all nodes. Among other things,
# this will set up the APT configuration, so make sure that the
# group_vars are reflecting the network setup. Here we also create
# a physical bridge br-phys if needed 
#
- name: Basic node setup 
  hosts: all
  become: yes
  vars_files:
    - global_vars.yaml
  roles:
    - nodeSetup


# The next three plays set up our VXLAN network that we will 
# present to OpenStack Neutron as a physical provider network
# The network will act as a virtual bridge, the compute nodes
# will attach to this bridge. Finally, we set up the network node
# so that it acts as a router between this network and the 
# actual physical network

- name: Create and wire up br-ext on network node
  hosts: network_nodes
  become: yes
  vars_files:
    global_vars.yaml
  vars:
    bridge_name: "{{phys_interface}}"
    patch_port_local_name: "patch-{{phys_bridge_name}}"
    patch_port_peer_name: "patch-{{phys_interface}}"
    vxlan_nodes: "{{ groups.compute_nodes  | map('extract', hostvars, 'underlay_ip') | list}}"
    vxlan_id: 100
  roles:
    - vxlan_bridge

- name: Create and wire up br-ext on compute nodes
  hosts: compute_nodes
  become: yes
  vars_files:
    global_vars.yaml
  vars:
    bridge_name: "{{phys_interface}}"
    patch_port_local_name: "patch-{{phys_bridge_name}}"
    patch_port_peer_name: "patch-{{phys_interface}}"
    vxlan_peer: "{{hostvars.network.underlay_ip}}"
    vxlan_id: 100
  roles:
    - vxlan_node
     

- name: Establish routing on network node to provide external connectivity for instances 
  hosts: network_nodes
  become: yes
  vars_files:
    global_vars.yaml
  vars:
    public_interface: "{{router_public_interface}}"
    internal_interface: "{{phys_interface}}"
    internal_ip_address: "172.16.0.1/24"
    internal_interface_mtu: 1450
  roles:
    - router


#
# Set up the controller node as NTP server
#
- name: Install NTP on controller node
  hosts: controller_nodes
  become: yes
  vars_files:
    global_vars.yaml
  vars:
    ntp_network_cidr: "{{management_network_cidr}}"
  roles:
    - ntpServer

#
# Set up the compute nodes, network nodes and storage nodes as NTP clients
#
- name: Install NTP client on other nodes nodes
  hosts: compute_nodes, network_nodes, storage_nodes
  become: yes
  vars_files:
    global_vars.yaml
  roles:
    - ntpClient


#
# Install MariaDB, Memcached and RabbitMQ on the controller
#
- name: Install middleware on controller node
  hosts: controller_nodes
  become: yes
  vars_files:
    global_vars.yaml
  vars:
    mariadb_server_ip: "{{hostvars[db_node].mgmt_ip}}"
    mariadb_root_password: "{{MARIADB_ROOT_PASSWORD}}"
    rabbit_user_name: "openstack"
    rabbit_user_password: "{{OS_SERVICE_PASSWORD}}"
    memcached_server_ip: "{{hostvars[memcached_node].mgmt_ip}}"
  roles:
    - mariaDB
    - rabbitMQ
    - memcached


#
# Install Keystone on the controller node
#
- name: Install Keystone on controller node
  hosts: controller_nodes
  become: yes
  vars_files:
    global_vars.yaml
  vars:
    keystone_db_user_password: "{{OS_SERVICE_PASSWORD}}"
  roles:
    - keystone


#
# Install Glance on the controller node
#
- name: Install Glance on controller node
  hosts: controller_nodes
  become: yes
  vars_files:
    global_vars.yaml
  vars:
    glance_db_user_password: "{{OS_SERVICE_PASSWORD}}"
    keystone_admin_password: "{{OS_ADMIN_PASSWORD}}"
    glance_keystone_user_password: "{{OS_SERVICE_PASSWORD}}"
  roles:
    - glance

#
# Install Placement on the controller node
#
- name: Install Placement on controller node
  hosts: controller_nodes
  become: yes
  vars_files:
    global_vars.yaml
  vars:
    keystone_admin_password: "{{OS_ADMIN_PASSWORD}}"
    placement_db_user_password: "{{OS_SERVICE_PASSWORD}}"
    placement_keystone_user_password: "{{OS_SERVICE_PASSWORD}}"
  roles:
    - placement

# 
# Install the Cinder API server and the Cinder scheduler
# on the controller node - this is the Cinder "control plane"
#
- name: Install Cinder on controller node
  hosts: controller_nodes
  become: yes
  vars_files:
    global_vars.yaml
  vars:
    cinder_db_user_password: "{{OS_SERVICE_PASSWORD}}"
    cinder_keystone_user_password: "{{OS_SERVICE_PASSWORD}}"
    keystone_admin_password: "{{OS_ADMIN_PASSWORD}}"
    rabbitmq_password: "{{OS_SERVICE_PASSWORD}}"
  roles:
    - cinder_server

#
# Install Cinder volume manager on the storage nodes
#
- name: Install Cinder on storage nodes
  hosts: storage_nodes
  become: yes
  vars_files:
    global_vars.yaml
  vars:
    cinder_db_user_password: "{{OS_SERVICE_PASSWORD}}"
    cinder_keystone_user_password: "{{OS_SERVICE_PASSWORD}}"
    rabbitmq_password: "{{OS_SERVICE_PASSWORD}}"
  roles:
    - cinder_node

#
# Install Nova on the controller node
#
- name: Install Nova on controller node
  hosts: controller_nodes
  become: yes
  vars_files:
    global_vars.yaml
  vars:
    neutron_keystone_user_password: "{{OS_SERVICE_PASSWORD}}"
    rabbitmq_password: "{{OS_SERVICE_PASSWORD}}"
    keystone_admin_password: "{{OS_ADMIN_PASSWORD}}"
    nova_db_user_password: "{{OS_SERVICE_PASSWORD}}"
    nova_keystone_user_password: "{{OS_SERVICE_PASSWORD}}"
    placement_keystone_user_password: "{{OS_SERVICE_PASSWORD}}"
    metadata_shared_secret: "{{OS_SHARED_SECRET}}"
    cinder_os_region_name: "RegionOne"
  roles:
    - nova


#
# Install Nova on the compute nodes
#
- name: Install Nova on compute nodes
  hosts: compute_nodes
  become: yes
  vars_files:
    global_vars.yaml
  vars:
    neutron_keystone_user_password: "{{OS_SERVICE_PASSWORD}}"
    rabbitmq_password: "{{OS_SERVICE_PASSWORD}}"
    keystone_admin_password: "{{OS_ADMIN_PASSWORD}}"
    nova_db_user_password: "{{OS_SERVICE_PASSWORD}}"
    nova_keystone_user_password: "{{OS_SERVICE_PASSWORD}}"
    placement_keystone_user_password: "{{OS_SERVICE_PASSWORD}}"
  roles:
    - nova_compute


#
# Now make sure that the Nova server knows about our
# compute nodes
#
- name: Discover compute hosts
  hosts: controller_nodes
  become: yes
  become_user: nova
  tasks:
    - name: Discover compute hosts
      shell:
        nova-manage cell_v2 discover_hosts


#
# Now we install the control plane components of 
# Neutron on our controller
#
- name: Install Neutron on controller nodes
  hosts: controller_nodes
  become: yes
  vars_files:
    global_vars.yaml
  vars:
    neutron_db_user_password: "{{OS_SERVICE_PASSWORD}}"
    neutron_keystone_user_password: "{{OS_SERVICE_PASSWORD}}"
    nova_keystone_user_password: "{{OS_SERVICE_PASSWORD}}"
    keystone_admin_password: "{{OS_ADMIN_PASSWORD}}"
    rabbitmq_password: "{{OS_SERVICE_PASSWORD}}"
  roles:
    - neutron_server

#
#  All agents (DHCP,Metadata) will run on the
# network node
#
- name: Install Neutron agents on network node
  hosts: network_nodes
  become: yes
  vars_files:
    global_vars.yaml
  vars:
    metadata_shared_secret: "{{OS_SHARED_SECRET}}"
    rabbitmq_password: "{{OS_SERVICE_PASSWORD}}"
    neutron_db_user_password: "{{OS_SERVICE_PASSWORD}}"
    neutron_keystone_user_password: "{{OS_SERVICE_PASSWORD}}"
    nova_keystone_user_password: "{{OS_SERVICE_PASSWORD}}"    
  roles:
    - neutron_agents

- name: Install L3 agent 
  hosts: network_nodes
  become: yes
  vars_files:
    global_vars.yaml
  roles:
    - neutron_l3agent

#
# On the compute nodes, we need the OVS agents
#
- name: Finalize Neutron configuration on compute nodes
  hosts: compute_nodes
  become: yes
  vars_files:
    global_vars.yaml
  vars:
    keystone_admin_password: "{{OS_ADMIN_PASSWORD}}"
    rabbitmq_password: "{{OS_SERVICE_PASSWORD}}"
    neutron_keystone_user_password: "{{OS_SERVICE_PASSWORD}}"
  roles:
    - neutron_compute

#
# Install Horizon on the controller node
#
- name: Install Horizon
  hosts: controller_nodes
  become: yes
  vars_files:
    global_vars.yaml
  vars:
    horizon_physical_networks: "['physnet']"
    horizon_supported_network_types: "['flat', 'vxlan']"
    horizon_enable_router: True
  roles:
    - horizon





